---
layout: post
title:  "Recreating VINN: The Surprising Effectiveness of Representation Learning for Visual Imitation"
date:   2024-10-22 12:00:00 +10
categories: robotics arm mujoco manipulation trajectory imitation learning deep neural network non-parametric nearest neighbour
comments: true
excerpt: Reimplementing the VINN paper, a non-parametric way of behaviour cloning direct from data.
assets: /assets/images/VINN/
preview-image: "/assets/images/VINN/200-random-1-gripper.gif"
---

Deep learning a robotic manipulation policy from scratch can be a difficult task, as it requires nailing [the algorithm, the implementation, the model and the data](https://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html). Compounded by the delay in the debugging cycle due to model training time, this 4-dimensional search for a good solution makes machine learning a pain in the back compared to traditional software development, which requires only iteration on the algorithm and implementation with instantaneous feedback. Typically, you need a good starting point and good intuition to make inroads into ML research.

<div style="text-align: center;">
    <img src="{{page.assets}}/ml-hard.png" style="width: 65%">
</div>
<div style="text-align: center;" class="caption"> The first 3 dimensions of ML development - then multiply by the 4th data dimension. From <a href="https://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html" target="_blank">here.</a></div>

In imitation learning for robotic manipulation, we are also usually solving two coupled problems: learning good visual features, and learning good actions.

In the paper [_The Surprising Effectiveness of Representation Learning for Visual Imitation_](https://arxiv.org/abs/2112.01511) (Pari et al., 2021), or VINN (Visual Imitation through Nearest Neighbors) for short, a simple solution is provided to break down the complexity of these steps. While it may not be as flexible and high-performing as state of the art deep-learnt methods, it is a great way to check that the environment, data and visual features are good before moving on to more sophisticated methods. It also forms a great baseline for the performance of subsequent methods.

# The VINN method
- vision decoupled from actions
- allows reuse of vision embeddings without retraining
- actions are nearest neighbours

<div style="text-align: center;">
    <img src="{{page.assets}}/method.png" style="width: 100%">
</div>
<div style="text-align: center;" class="caption"> Overview of the VINN method. From <a href="https://jyopari.github.io/VINN/" target="_blank">VINN.</a></div>


At the highest level, given a dataset of expert demonstrations of a task, the vision learning is separated from the action learning. Visual features are fine tuned on the dataset from an ImageNet-pretrained ResNet model with the amazingly simple self-supervised [Bootstrap Your Own Latent (BYOL)](https://arxiv.org/abs/2006.07733) method. Fine-tuning the visual encoder on the data distribution found in the task is an important step for good quality visual features, as there is usually a significant domain shift from ImageNet data to robot data. The beauty of BYOL is that it learns good features through data augmentation (no labels or contrastive pairs needed), thus it requires no work aside from just going and training your vision model on the dataset - instantly applicable to any task!

Another major benefit of the separation of vision learning is that you can iterate on changing/improving the actions without having to retrain the vision encoder each time, as long as the environment/objects/positions involved are the same. This allows you to add more data, change the action type (i.e. joint positions vs cartesian pose, relative vs world frame), or even try different tasks without once waiting for a vision model to retrain, a huge time and compute saver.

On the action side, VINN keeps things really simple by avoiding learning altogether and simply doing a nearest neighbour search in the dataset. That is, at test time, the closest visual match for the current vision input in the dataset is found (i.e. the nearest neighbour), and the corresponding action is chosen.

This is achieved by first embedding all the vision data into vector space with the vision encoder, and saving the vector. Then at test time, the current visual input is then also encoded, and a nearest neighbour vector search is done. Simple, but effective!

# My implementation

Because the method is so simple, it was honestly easier to reimplement for my environment than to adapt from the [authors' repository](https://github.com/jyopari/VINN/tree/main).

